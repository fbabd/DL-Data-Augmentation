{
 "cells": [
  
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123) \n",
    "import pickle \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset \n",
    "torch.manual_seed(42) \n",
    "from torchsummary import summary \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data = data.astype('float32') / 255.0\n",
    "    data = data.reshape((-1, 3, 32, 32)) # pytorch dimension = (B, C, H, W)\n",
    "    mean = np.mean(data, axis=(0, 1, 2))\n",
    "    std = np.std(data, axis=(0, 1, 2))\n",
    "    data = (data - mean) / std\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, train_loader, test_loader, ITR=100, data_aug = 'None', alpha=0.2):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = ITR\n",
    "\n",
    "    train_loss_values = []\n",
    "    train_acc_values = []\n",
    "    test_loss_values = []\n",
    "    test_acc_values = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            if data_aug == 'cutout':\n",
    "                images = apply_cutout_minibatch(images.detach().clone(), 16)\n",
    "            if data_aug == 'mixup':\n",
    "                images, labels = apply_mixup_minibatch(images.detach().clone(), labels.detach().clone(), alpha)\n",
    "            if data_aug == 'standard':\n",
    "                images = apply_standard_minibatch(images.detach().clone(), 4) \n",
    "            if data_aug == 'all':\n",
    "                images = apply_standard_minibatch(images.detach().clone(), 4) \n",
    "                images = apply_cutout_minibatch(images.detach().clone(), 16) \n",
    "                images, labels = apply_mixup_minibatch(images.detach().clone(), labels.detach().clone(), 0.2) \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0) \n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "        train_loss_values.append(train_loss)\n",
    "        train_acc_values.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        test_accuracy = correct / total\n",
    "        test_loss_values.append(test_loss)\n",
    "        test_acc_values.append(test_accuracy)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.3f}, Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.3f}\")\n",
    "\n",
    "    history = {}\n",
    "    history['train_acc'] = train_acc_values  \n",
    "    history['train_loss'] = train_loss_values \n",
    "    history['test_acc'] = test_acc_values  \n",
    "    history['test_loss'] = test_loss_values \n",
    "\n",
    "    return history \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, figsize=(12,5), title = 'Training History'):\n",
    "    num_epochs = len(history['train_loss'])\n",
    "    plt.figure(figsize=figsize)\n",
    "    a = plt.subplot(1, 2, 1)\n",
    "    a.plot(range(1, num_epochs+1), history['train_loss'], label='Train Loss')\n",
    "    a.plot(range(1, num_epochs+1), history['test_loss'], label='Test Loss')\n",
    "    a.set_xlabel('Epoch')\n",
    "    a.set_ylabel('Loss')\n",
    "    a.set_title('Training and Test Loss')\n",
    "    a.legend() \n",
    "\n",
    "    b = plt.subplot(1, 2, 2)\n",
    "    b.plot(range(1, num_epochs+1), history['train_acc'], label='Train Acc')\n",
    "    b.plot(range(1, num_epochs+1), history['test_acc'], label='Test Acc')\n",
    "    b.set_xlabel('Epoch')\n",
    "    b.set_ylabel('Accuracy')\n",
    "    b.set_title('Training and Test Accuracy')\n",
    "    b.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show() \n",
    "    \n",
    "    print(f\"Final Test Accuracy is {history['test_acc'][-1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)  \n",
    "from resnet20 import ResNet, BasicBlock \n",
    "model = ResNet(BasicBlock, [3, 3, 3], num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n=1000):\n",
    "    data_path = 'cifar-10-batches-py/'\n",
    "\n",
    "    train_data = np.empty((50000, 3072), dtype=np.uint8) \n",
    "    train_labels = np.empty((50000,), dtype=np.int64) \n",
    "    for i in range(1, 6):\n",
    "        train_batch = unpickle(data_path + 'data_batch_' + str(i))\n",
    "        train_data[(i - 1) * 10000: i * 10000, :] = train_batch[b'data']\n",
    "        train_labels[(i - 1) * 10000: i * 10000] = train_batch[b'labels']\n",
    "        \n",
    "\n",
    "    test_batch = unpickle(data_path + 'test_batch')\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = np.array(test_batch[b'labels']) \n",
    "\n",
    "    # Sample n examples uniformly at random for each class from the training set\n",
    "    classes = np.unique(train_labels)\n",
    "    sampled_train_data = []\n",
    "    sampled_train_labels = []\n",
    "\n",
    "    for class_label in classes:\n",
    "        indices = np.where(train_labels == class_label)[0]\n",
    "        np.random.shuffle(indices)\n",
    "        sampled_indices = indices[:n]   \n",
    "        sampled_train_data.extend(train_data[sampled_indices])\n",
    "        sampled_train_labels.extend(train_labels[sampled_indices])\n",
    "\n",
    "    indices = np.array(range(len(sampled_train_data)))\n",
    "    np.random.shuffle(indices) \n",
    "    sampled_train_data = np.array(sampled_train_data)[indices]\n",
    "    sampled_train_labels = np.array(sampled_train_labels)[indices] \n",
    "\n",
    "    # normalize features (zero mean and unit variance)  \n",
    "    sampled_train_data = preprocess_data(sampled_train_data)\n",
    "    test_data = preprocess_data(test_data) \n",
    "\n",
    "    return sampled_train_data, test_data, sampled_train_labels, test_labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train_data, test_data, sampled_train_labels, test_labels = get_data() \n",
    "\n",
    "print(\"Sampled Train Data Shape:\", sampled_train_data.shape)\n",
    "print(\"Sampled Train Labels Shape:\", sampled_train_labels.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)\n",
    "print(\"Test Labels Shape:\", test_labels.shape)\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(sampled_train_data), torch.from_numpy(sampled_train_labels))\n",
    "test_dataset = TensorDataset(torch.from_numpy(test_data), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 64  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sampled_train_data[3].transpose( 1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (3 pts) Train your Resnet model without augmentation and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = run_model(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (4 pts) Implement mixup and report the results for α = 0.2 and α = 0.4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(img1, img2, lb1, lb2, alpha_value):\n",
    "    lam = np.random.beta(alpha_value, alpha_value) \n",
    "    mixed_image = lam * img1 + (1 - lam) * img2  \n",
    "    mixed_label = lam * lb1 + (1 - lam) * lb2\n",
    "    mixed_image = np.array(mixed_image)\n",
    "    mixed_label = np.array(mixed_label) \n",
    "\n",
    "    return mixed_image, mixed_label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mixup_minibatch(minibatch_images, minibatch_labels, mask_size):\n",
    "    for i in range(minibatch_images.shape[0]): \n",
    "        idx = np.random.randint(minibatch_images.shape[0]) \n",
    "        img1 = minibatch_images[i]\n",
    "        img2 = minibatch_images[idx]\n",
    "        lb1 = minibatch_labels[i] \n",
    "        lb2 = minibatch_labels[idx] \n",
    "        new_im, new_lb = mixup(img1, img2, lb1, lb2, mask_size) \n",
    "        minibatch_images[i] = torch.from_numpy(new_im).float()\n",
    "        minibatch_labels[i] = torch.from_numpy(new_lb).float() \n",
    "        \n",
    "    return minibatch_images, minibatch_labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train_data, test_data, sampled_train_labels, test_labels = get_data()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=3\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(sampled_train_data[i].transpose( 1, 2, 0))  \n",
    "\n",
    "plt.subplot(122)\n",
    "im, lb = mixup(sampled_train_data[i].copy(), sampled_train_data[i*2].copy(), sampled_train_labels[i].copy(), sampled_train_labels[i*2].copy(), 0.2)\n",
    "plt.imshow(im.transpose( 1, 2, 0))   \n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history2 = run_model(model, train_loader, test_loader, data_aug='mixup', alpha=0.2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history22 = run_model(model, train_loader, test_loader, data_aug='mixup', alpha=0.4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history22) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (4 pts) Cutout augmentation (K = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout(image, mask_size):\n",
    "    if np.random.rand() < 0.5:\n",
    "        return image\n",
    "\n",
    "    channels, height, width = image.shape\n",
    "\n",
    "    center_y = np.random.randint(0, height)\n",
    "    center_x = np.random.randint(0, width)\n",
    "\n",
    "    half_size = mask_size // 2\n",
    "    top = max(0, center_y - half_size)\n",
    "    bottom = min(height, center_y + half_size)\n",
    "    left = max(0, center_x - half_size)\n",
    "    right = min(width, center_x + half_size)\n",
    "\n",
    "    image[:, top:bottom, left:right] = 0\n",
    "\n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cutout_minibatch(minibatch_images, mask_size):\n",
    "    for i in range(minibatch_images.shape[0]):\n",
    "        minibatch_images[i] = cutout(minibatch_images[i], mask_size)\n",
    "    return minibatch_images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train_data, test_data, sampled_train_labels, test_labels = get_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=3 \n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(sampled_train_data[i].transpose( 1, 2, 0))  \n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(cutout(sampled_train_data[i].copy(), 16).transpose( 1, 2, 0))   \n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history3 = run_model(model, train_loader, test_loader, data_aug='cutout')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. (4 pts)  Standard augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard(image, K):\n",
    "    k1 = np.random.randint(-K, K+1)\n",
    "    k2 = np.random.randint(-K, K+1)\n",
    "    shifted_image = np.zeros_like(image)\n",
    "    if k1 >= 0 and k2 >= 0:\n",
    "        shifted_image[:, :image.shape[1]-k1, :image.shape[2]-k2] = image[:, k1:, k2:]\n",
    "    elif k1 >= 0 and k2 < 0:\n",
    "        shifted_image[:, :image.shape[1]-k1, -k2:] = image[:, k1:, :image.shape[2]+k2]\n",
    "    elif k1 < 0 and k2 >= 0:\n",
    "        shifted_image[:, -k1:, :image.shape[2]-k2] = image[:, :image.shape[1]+k1, k2:]\n",
    "    else:\n",
    "        shifted_image[:, -k1:, -k2:] = image[:, :image.shape[1]+k1, :image.shape[2]+k2]\n",
    "    \n",
    "    if np.random.rand() < 0.5:\n",
    "        flipped_image = np.flip(shifted_image, axis=2)\n",
    "    else:\n",
    "        flipped_image = shifted_image \n",
    "\n",
    "    return flipped_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train_data, test_data, sampled_train_labels, test_labels = get_data()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(sampled_train_data[i].transpose( 1, 2, 0))  \n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(standard(sampled_train_data[i].copy(), 10).transpose( 1, 2, 0))   \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_standard_minibatch(minibatch_images, K):\n",
    "    for i in range(minibatch_images.shape[0]):\n",
    "        std_img = standard(minibatch_images[i], K) \n",
    "        minibatch_images[i] = torch.from_numpy(std_img.copy()).float()\n",
    "    return minibatch_images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history4 = run_model(model, train_loader, test_loader, data_aug='standard')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. (3 pts) Combine all augmentations together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history5 = run_model(model, train_loader, test_loader, data_aug='all')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history5) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Does combining improve things further? \n",
    "\n",
    "-> No. Combining all three augmentation does not improve further. Becasue, we have included augmentation which brings too much randomness to the dataset. For that reason, combining the augementation is not the best way to train model in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "\n",
    "c = plt.subplot(2, 2, 1)\n",
    "c.plot(range(1, 101), history1['train_acc'], label='No Augmentation')\n",
    "c.plot(range(1, 101), history2['train_acc'], label='Mixup (alpha=0.2)') \n",
    "c.plot(range(1, 101), history22['train_acc'], label='Mixup (alpha=0.4)')\n",
    "c.plot(range(1, 101), history3['train_acc'], label='Cutout') \n",
    "c.plot(range(1, 101), history4['train_acc'], label='Standard (Shift+Flip)') \n",
    "c.plot(range(1, 101), history5['train_acc'], label='All Augmentation') \n",
    "c.set_xlabel('Epoch')\n",
    "c.set_ylabel('Train Accuracy')\n",
    "c.set_title('Train Accuracy for different augmentation')\n",
    "c.legend() \n",
    "\n",
    "b = plt.subplot(2, 2, 2)\n",
    "b.plot(range(1, 101), history1['test_acc'], label='No Augmentation')\n",
    "b.plot(range(1, 101), history2['test_acc'], label='Mixup (alpha=0.2)') \n",
    "b.plot(range(1, 101), history22['test_acc'], label='Mixup (alpha=0.4)')\n",
    "b.plot(range(1, 101), history3['test_acc'], label='Cutout') \n",
    "b.plot(range(1, 101), history4['test_acc'], label='Standard (Shift+Flip)') \n",
    "b.plot(range(1, 101), history5['test_acc'], label='All Augmentation') \n",
    "b.set_xlabel('Epoch')\n",
    "b.set_ylabel('Test Accuracy')\n",
    "b.set_title('Test Accuracy for different augmentation')\n",
    "b.legend()  \n",
    "\n",
    "a = plt.subplot(2, 2, 3) \n",
    "a.plot(range(1, 101), history1['train_loss'], label='No Augmentation')\n",
    "a.plot(range(1, 101), history2['train_loss'], label='Mixup (alpha=0.2)') \n",
    "a.plot(range(1, 101), history22['train_loss'], label='Mixup (alpha=0.4)')\n",
    "a.plot(range(1, 101), history3['train_loss'], label='Cutout') \n",
    "a.plot(range(1, 101), history4['train_loss'], label='Standard (Shift+Flip)') \n",
    "a.plot(range(1, 101), history5['train_loss'], label='All Augmentation') \n",
    "a.set_xlabel('Epoch')\n",
    "a.set_ylabel('Train Loss')\n",
    "a.set_title('Training Loss for different augmentation')\n",
    "a.legend() \n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.title(title) \n",
    "plt.show() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. (2 pts) Comment on the role of data augmentation. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How does it affect test accuracy, train accuracy and the convergence of optimization? Is test accuracy higher? Does training loss converge faster? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the observed plots, it can be concluded that:\n",
    "\n",
    "- Without augmentation, the model tends to overfit the training data.\n",
    "\n",
    "- Mixup augmentation leads to lower train and test accuracies. This augmentation introduces excessive randomness, making it difficult for the model to learn meaningful features. However, when comparing different mixup values, alpha = 0.2 performs relatively better than alpha = 0.4. Both values reach a plateau in terms of optimization, indicating that this augmentation technique is not suitable for this dataset.\n",
    "\n",
    "- Cutout augmentation shows improvements compared to the base case. The difference between train and test accuracies is smaller, indicating reduced overfitting. The training loss initially decreases quickly, and then the performance plateaus.\n",
    "\n",
    "- Standard augmentation performs the best among the techniques evaluated. It exhibits the least overfitting, with the highest test accuracy. Both train and test accuracies improve over time, and the training loss continuously decreases.\n",
    "\n",
    "- Augmenting with a combination of techniques, including mixup, does not yield good performance. Mixup introduces excessive randomness, hindering the model's ability to find patterns. Although the accuracies and losses are better than with mixup alone, this technique does not compare favorably to the others.\n",
    "\n",
    "Overall, the standard augmentation technique performs the best. However, it is worth noting that if the model were trained for additional epochs, it could potentially achieve even better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
